1. Rotary Positional Embedding (RoPE)
Su, J., et al. (2021).
RoFormer: Enhanced Transformer with Rotary Position Embedding.
https://arxiv.org/abs/2104.09864

2. Mixture of Experts (MoE)
Shazeer, N., et al. (2017).
Outrageously Large Neural Networks: The Sparsely-Gated MoE Layer.
https://arxiv.org/abs/1701.06538

3. Load Balancing (Switch Transformers)
Fedus, W., et al. (2021).
Switch Transformers: Scaling to Trillion Parameter Models.
https://arxiv.org/abs/2101.03961